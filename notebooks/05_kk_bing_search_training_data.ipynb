{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.cosmos import GremlinQueryManager, DocumentQueryManager\n",
    "from src.data.graph.gremlin import GremlinQueryBuilder\n",
    "from src.data.cloud._constants import Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "\n",
    "account_name = os.environ.get('COSMOS_ACCOUNT_NAME')\n",
    "db_name = os.environ.get('COSMOS_DB_NAME')\n",
    "graph_name = os.environ.get('COSMOS_GRAPH_NAME')\n",
    "master_key = os.environ.get('COSMOS_MASTER_KEY')\n",
    "\n",
    "graph_name = 'main'\n",
    "\n",
    "gremlin_qm = GremlinQueryManager(account_name, master_key, db_name, graph_name)\n",
    "doc_qm = DocumentQueryManager(account_name, master_key, db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alibaba',\n",
       " 'Amazon Web Services',\n",
       " 'Microsoft Azure',\n",
       " 'DigitalOcean',\n",
       " 'Google Cloud',\n",
       " 'IBM Cloud',\n",
       " 'Oracle Cloud']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gremlin_qm.query('g.V().has(\"label\", \"cloud\").values(\"name\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Azure Bot Service',\n",
       " 'Azure Databricks',\n",
       " 'Azure Search',\n",
       " 'Bing Autosuggest',\n",
       " 'Bing Custom Search']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cloud_data = []\n",
    "services_names = {}\n",
    "\n",
    "abbrs = gremlin_qm.query('g.V().has(\"label\", \"cloud\").values(\"abbreviation\")')\n",
    "for abbr in abbrs:\n",
    "    q = f\"\"\"g.V().has(\"label\", \"{abbr}_service\")\n",
    "            .project(\"id\", \"name\", \"shortDescription\", \"longDescription\", \"uri\", \"iconUri\", \"categories\", \"relatedServices\")\n",
    "            .by(\"id\").by(\"name\").by(\"short_description\").by(\"long_description\").by(\"uri\").by(\"icon_uri\")\n",
    "            .by(out(\"belongs_to\").project(\"id\", \"name\").by(values(\"id\")).by(values(\"name\")).fold())\n",
    "            .by(coalesce(out(\"related_service\").project(\"id\", \"name\").by(values(\"id\")).by(values(\"name\")).fold(), __.not(identity()).fold()))\"\"\"\n",
    "    cloud_data = gremlin_qm.query(q)\n",
    "    \n",
    "    all_cloud_data += cloud_data\n",
    "    services_names[abbr] = [s['name'] for s in cloud_data]\n",
    "    \n",
    "services_names['azure'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '4ed0abb5-43af-4d8f-927d-2920f2a44d54', 'name': 'Azure Functions', 'shortDescription': 'Process events with serverless code', 'longDescription': 'Improve end-to-end development experience using a serverless compute platform with automated, flexible scaling. Choose programming languages and hosting options that best fit your business.', 'uri': 'https://azure.microsoft.com/en-us/services/functions/', 'iconUri': '', 'categories': [{'id': 'bd05cb77-2b71-4d9f-a97b-c0e6e1343155', 'name': 'Compute'}, {'id': '946a525d-5ee9-40b9-9aaf-9789527a7587', 'name': 'Containers'}, {'id': '3d0e3bbb-5ef0-42db-bbac-a43f8472dc5d', 'name': 'Internet of Things'}], 'relatedServices': [{'id': 'eed8e9c5-2b9d-41ee-a9d1-2e20b53bc058', 'name': 'AWS Lambda'}, {'id': '1644b196-5c0b-4518-81e1-c3e77ca66332', 'name': 'Google Cloud Functions'}]}\n"
     ]
    }
   ],
   "source": [
    "for s in all_cloud_data:\n",
    "    if s['name'] == 'Azure Functions':\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Training Data from Bing using Apify crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apify_training_data(terms: List[str], suffix: str = ''):\n",
    "    output_urls = []\n",
    "    errors = []\n",
    "    for i, s in enumerate(terms):\n",
    "        if i > 1 and i % 30 == 0:\n",
    "            print(f'{i}/{len(terms)} terms searched')\n",
    "            time.sleep(10)\n",
    "        try:\n",
    "            res = requests.post(\n",
    "                'https://api.apify.com/v2/acts/G4B5HdLYCXDyiafhy/runs?token=BDR3StxrybGSpod9z5Ara2o83',\n",
    "                json={\n",
    "                    'q': f'{s} {suffix}'\n",
    "                }\n",
    "            )\n",
    "            res.raise_for_status()\n",
    "            kv_store = res.json()['data']['defaultKeyValueStoreId']\n",
    "            output_url = f\"https://api.apify.com/v2/key-value-stores/{kv_store}/records/OUTPUT?disableRedirect=true\"\n",
    "            output_urls.append({'name': s, 'output_url': output_url})\n",
    "            \n",
    "        except:\n",
    "            print(f'Error crawling term: {s}')\n",
    "            errors.append(s)\n",
    "    return output_urls, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_output_urls = {}\n",
    "for abbr, svc_names in services_names.items():\n",
    "    print(f'Getting NER training data for: {abbr}')\n",
    "    cloud_output_urls[abbr] = apify_training_data(svc_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'name': 'Amazon MQ',\n",
       "   'output_url': 'https://api.apify.com/v2/key-value-stores/dL3JdN4SPZJtjJcha/records/OUTPUT?disableRedirect=true'}],\n",
       " [])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apify_training_data(['Amazon MQ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "['Amazon MQ', 'Amazon AppSync', 'AWS Cost Explorer', 'Reserved Instance Reporting', 'AWS Cost and Usage Report', 'Amazon EC2 Auto Scaling', 'AWS DeepLens', 'AWS Ground Station']\n",
      "[]\n",
      "['Azure SQL Database', 'Azure Cosmos DB', 'Azure IoT Edge', 'Azure Spatial Anchors', 'Azure Visual Studio App Center', 'Azure DNS', 'Azure Virtual Network', 'Azure Load Balancer', 'Bing Autosuggest', 'Bing Custom Search', 'Bing Entity Search', 'Bing Image Search', 'Azure Video Indexer', 'Azure Kinect DK', 'Azure Stream Analytics', 'Azure SQL Data Warehouse', 'Azure HDInsight']\n",
      "[]\n",
      "['Kubernetes', 'Spaces Object Storage', 'Volumes Block Storage', 'Cloud Firewalls', 'Load Balancers', 'Floating IPs', 'DNS', 'DigitalOcean API', 'DigitalOcean CLI', 'Monitoring']\n",
      "[]\n",
      "['Compute Engine', 'Cloud Services Platform', 'Apigee Sense', 'Apigee healthcare APIx', 'Apigee Open Banking APIx', 'Cloud Healthcare API', 'Google Data Studio*', 'Cloud SQL', 'Cloud Bigtable', 'Cloud Spanner', 'Profiler (beta)', 'Transparent Service Level Indicators', 'Cloud IAM', 'Cloud Identity for Customers and Partners (beta)', 'Cloud Identity-Aware Proxy', 'Drive Enterprise']\n",
      "[]\n",
      "['Content Delivery Network (CDN)', 'Direct Link', 'Domain Name Service (DNS)', 'Block Storage', 'File Storage', 'IBM Cloud Backup', 'Availability Monitoring', 'Cloud Automation Manager', 'Cloud Event Management', 'Continuous Delivery', 'DevOps Insights', 'Cloud CLI']\n",
      "[]\n",
      "['GPU-based Virtual Machines and Bare Metal', 'Virtual Machines', 'Block Volume', 'Key Management Service (KMS)', 'Domain Name System (DNS)', 'Oracle Blockchain Cloud Service', 'Oracle Digital Assistant', 'Oracle Mobile Cloud', 'Oracle Developer Cloud Service', 'Oracle Autonomous NoSQL Database Cloud Service', 'Oracle API Platform Cloud Service ', 'Oracle CASB Cloud Service', 'Oracle Configuration and Compliance Cloud Service', 'Oracle Identity Cloud Service', 'Oracle Application Performance Monitoring Cloud Service', 'Oracle Infrastructure Monitoring Cloud Service', 'Oracle IT Analytics Cloud Service']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for abbr, output in cloud_output_urls.items():    \n",
    "    output_urls, errors = apify_training_data(output[1])\n",
    "    _output_urls = cloud_output_urls[abbr][0]\n",
    "    _errors = cloud_output_urls[abbr][1]\n",
    "    cloud_output_urls[abbr] = (output_urls + _output_urls, errors + _errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_output_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./cloud_output_urls_training_data.json', 'w+') as f:\n",
    "    json.dump(cloud_output_urls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alibaba\n",
      "aws\n",
      "azure\n",
      "digitalocean\n",
      "gcp\n",
      "ibm\n",
      "oracle\n"
     ]
    }
   ],
   "source": [
    "cloud_ner_examples = {}\n",
    "for abbr, output in cloud_output_urls.items():\n",
    "    print(abbr)\n",
    "    data = [requests.get(o['output_url']).json() for o in output[0]]\n",
    "    cloud_ner_examples[abbr] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_ner_examples['oracle'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for abbr, data in cloud_ner_examples.items():\n",
    "    path = f'../data/processed/ner_training_examples/{abbr}'\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(f'{path}/bing_search_training_examples.jsonl', 'w+') as f:\n",
    "        for svc in data:\n",
    "            if 'webPages' in svc and 'news' in svc:\n",
    "#                 print(svc['webPages'])\n",
    "#                 raise\n",
    "                svc_data = svc['webPages'] + svc['news']\n",
    "                for svc_data in svc_data:\n",
    "                    if 'paragraphs' in svc_data:\n",
    "                        for p in svc_data['paragraphs']:\n",
    "                            f.write(json.dumps({\n",
    "                                'text': p,\n",
    "                                'meta': {\n",
    "                                    'source_url': svc_data['url']\n",
    "                                }\n",
    "                            }) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon MQ is a managed message broker service ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With Amazon MQ you can use the AWS Management ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazon MQ makes it easy to migrate messaging t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon MQ provides high availability and messa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amazon MQ offers low latency messaging, often ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SkipTheDishes lowered maintenance time and imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Malmberg improved messaging stability and redu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dealer.com migrated messaging to Amazon MQ, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bench Accounting improved broker resilience wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Implementing enterprise integration patterns w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Amazon MQ is a managed message broker service ...\n",
       "1  With Amazon MQ you can use the AWS Management ...\n",
       "2  Amazon MQ makes it easy to migrate messaging t...\n",
       "3  Amazon MQ provides high availability and messa...\n",
       "4  Amazon MQ offers low latency messaging, often ...\n",
       "5  SkipTheDishes lowered maintenance time and imp...\n",
       "6  Malmberg improved messaging stability and redu...\n",
       "7  Dealer.com migrated messaging to Amazon MQ, ta...\n",
       "8  Bench Accounting improved broker resilience wi...\n",
       "9  Implementing enterprise integration patterns w..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "texts = []\n",
    "with open ('../data/processed/ner_training_examples/aws/bing_search_training_examples.jsonl') as e_file:\n",
    "    for line in e_file.readlines()[:10]:\n",
    "        texts.append(json.loads(line)['text'])\n",
    "\n",
    "df = pd.DataFrame(texts, columns=['text'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/ner_training_examples/aws/bing_search_training_examples.tsv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pattern files for services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'alibaba': 'ALIBABA_CLOUD_SERVICE',\n",
    "    'aws': 'AWS_SERVICE',\n",
    "    'azure': 'AZURE_SERVICE',\n",
    "    'digitalocean': 'DIGITAL_OCEAN_SERVICE',\n",
    "    'gcp': 'GOOGLE_CLOUD_SERVICE',\n",
    "    'ibm': 'IBM_CLOUD_SERVICE',\n",
    "    'oracle': 'ORACLE_CLOUD_SERVICE'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['alibaba', 'aws', 'azure', 'digitalocean', 'gcp', 'ibm', 'oracle'])\n"
     ]
    }
   ],
   "source": [
    "print(services_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alibaba\n",
      "aws\n",
      "azure\n",
      "digitalocean\n",
      "gcp\n",
      "ibm\n",
      "oracle\n"
     ]
    }
   ],
   "source": [
    "all_patterns = []\n",
    "\n",
    "os.makedirs('../data/processed/ner_training_patterns', exist_ok=True)\n",
    "for abbr, services in services_names.items():\n",
    "    cloud_patterns = []\n",
    "    print(abbr)\n",
    "    path = f'../data/processed/ner_training_patterns/{abbr}/patterns.jsonl'\n",
    "    os.makedirs('/'.join(path.split('/')[:-1]), exist_ok=True)\n",
    "    \n",
    "    cloud_patterns.append({'label': labels[abbr], 'pattern': [{'LOWER': abbr}, {'IS_UPPER': True}]})\n",
    "    cloud_patterns.append({'label': labels[abbr], 'pattern': [{'LOWER': abbr}, {'IS_UPPER': True}, {'IS_UPPER': True}]})\n",
    "    cloud_patterns.append({'label': labels[abbr], 'pattern': [{'LOWER': abbr}, {'IS_UPPER': True}, {'IS_UPPER': True}, {'IS_UPPER': True}]})\n",
    "    for s in services:\n",
    "        split_ = s.split()\n",
    "        if len(split_) > 1:\n",
    "            p = [{'LOWER': w.lower()} for w in split_]\n",
    "        else:\n",
    "            p = s\n",
    "        pattern = {'label': labels[abbr], 'pattern': p, 'id': s}\n",
    "        cloud_patterns.append(pattern)\n",
    "    \n",
    "    all_patterns += cloud_patterns\n",
    "\n",
    "    with open(path, 'w+') as abbr_patterns_file:\n",
    "        for cp in cloud_patterns:\n",
    "            abbr_patterns_file.write(json.dumps(cp) + '\\n')\n",
    "\n",
    "with open('../data/processed/ner_training_patterns/base_patterns.jsonl', 'w+') as base_patterns_file:\n",
    "    for p in patterns:\n",
    "        base_patterns_file.write(json.dumps(p) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'ORACLE_CLOUD_SERVICE',\n",
       "  'pattern': [{'LOWER': 'oracle'}, {'IS_UPPER': True}]},\n",
       " {'label': 'ORACLE_CLOUD_SERVICE',\n",
       "  'pattern': [{'LOWER': 'oracle'}, {'IS_UPPER': True}, {'IS_UPPER': True}]},\n",
       " {'label': 'ORACLE_CLOUD_SERVICE',\n",
       "  'pattern': [{'LOWER': 'oracle'},\n",
       "   {'IS_UPPER': True},\n",
       "   {'IS_UPPER': True},\n",
       "   {'IS_UPPER': True}]},\n",
       " {'label': 'ORACLE_CLOUD_SERVICE',\n",
       "  'pattern': [{'LOWER': 'oracle'},\n",
       "   {'LOWER': 'bare'},\n",
       "   {'LOWER': 'metal'},\n",
       "   {'LOWER': 'compute'}],\n",
       "  'id': 'Oracle Bare Metal Compute'}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_patterns[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING FROM DISK ENTITY RULER:  /mnt/c/users/kakh/Documents/cloud_compete_graph/.venv/lib/python3.6/site-packages/en_ner_cloud_lg/en_ner_cloud_lg-0.2.0/entity_ruler\n",
      "<generator object read_jsonl at 0x7f5f7f10b728>\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_ner_cloud_lg')\n",
    "\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler.add_patterns(patterns)\n",
    "nlp.add_pipe(ruler, name='updated_ruler', after='ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entity_ruler', <spacy.pipeline.entityruler.EntityRuler at 0x7f5fce10c518>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.remove_pipe('entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.get_pipe('entity_ruler')\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer', 'tagger', 'parser', 'ner', 'updated_ruler']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.rename_pipe('updated_ruler', 'entity_ruler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('../models/en_ner_cloud_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('functions', 'AZURE_SERVICE'), ('AWS Lambda', 'AWS_SERVICE'), ('Azure Functions', 'AZURE_SERVICE'), ('Google Cloud Functions', 'GOOGLE_CLOUD_SERVICE'), ('IBM Cloud Functions', 'IBM_CLOUD_SERVICE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"You can create serverless functions on many different cloud platforms. The primary services are AWS Lambda, Azure Functions, Google Cloud Functions and IBM Cloud Functions.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Azure personalizer', 'AZURE_SERVICE')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Azure personalizer uses reinforcement learning to make intelligent recommendations')\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1604"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.get_pipe('entity_ruler').patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-8c4546df892d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'entity_ruler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "nlp.get_pipe('entity_ruler').patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('../models/prodigy_1.8-ner_cloud_lower_core_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Create cloud functions with Google Serverless]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloud functions GOOGLE_CLOUD_SERVICE\n",
      "Serverless GOOGLE_CLOUD_SERVICE\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "for ent in doc.ents:\n",
    "    if ent.label_.split('_')[0].lower() in ent.sent.text.lower():\n",
    "        print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "from src.app.exceptions import DocumentParseError\n",
    "from src.app.labels import LABELS\n",
    "\n",
    "\n",
    "class CloudServiceExtractor:\n",
    "    def __init__(self, search_client, model='en_ner_cloud_lg'):\n",
    "\n",
    "        self.search_client = search_client\n",
    "\n",
    "        print(\"Loading NER model...\", end=\"\")\n",
    "        self.nlp = spacy.load(model)\n",
    "        self.nlp.add_pipe(self.nlp.create_pipe('merge_entities'))\n",
    "        print(\"Done\")\n",
    "\n",
    "    async def resolve_service_name(self, name, ent_label, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Resolve the name of the service from the \n",
    "        NER model to the search index\n",
    "        \"\"\"\n",
    "        filter_ = f\"cloud eq '{LABELS[ent_label]}'\"\n",
    "        res = await self.search_client.suggest(name, filter_=filter_)\n",
    "\n",
    "        try:\n",
    "            res.raise_for_status()\n",
    "            suggestion = res.json()[\"value\"][0] if res.json()[\"value\"] else name\n",
    "\n",
    "            if isinstance(suggestion, str):\n",
    "                search_res = await self.search_client.search(name, filter_=filter_)\n",
    "            else:\n",
    "                search_res = await self.search_client.search(\n",
    "                    suggestion[\"@search.text\"], filter_=filter_\n",
    "                )\n",
    "            search_res.raise_for_status()\n",
    "            top_res = search_res.json()[\"value\"][0]\n",
    "            return top_res\n",
    "        except:\n",
    "            print(f\"Could not resolve: {name}\")\n",
    "            return None\n",
    "\n",
    "    async def extract(self, text, ent_labels=list(LABELS.keys())):\n",
    "        \"\"\"\n",
    "        Extract Named Entity Cloud services and relationships in text\n",
    "        \"\"\"\n",
    "        try:\n",
    "            doc = self.nlp(text)\n",
    "        except ValueError:\n",
    "            raise DocumentParseError\n",
    "\n",
    "        res = []\n",
    "        for ent in filter(lambda w: w.ent_type_ in ent_labels, doc):\n",
    "            service = await self.resolve_service_name(ent.text, ent.ent_type_)\n",
    "            if service:\n",
    "                relation = None\n",
    "                root_verb = None\n",
    "\n",
    "                if ent.dep_ in (\"attr\", \"dobj\"):\n",
    "                    subject = [w for w in ent.head.lefts if w.dep_ == \"nsubj\"]\n",
    "                    if subject:\n",
    "                        subject = subject[0]\n",
    "                        relation = subject\n",
    "\n",
    "                elif ent.dep_ == \"pobj\" and ent.head.dep_ == \"prep\":\n",
    "                    relation = ent.head.head\n",
    "                    cur = relation\n",
    "                    while cur.head:\n",
    "                        if cur.pos_ == \"VERB\":\n",
    "                            root_verb = cur\n",
    "                            break\n",
    "                        if cur.head == cur:\n",
    "                            break\n",
    "                        cur = cur.head\n",
    "\n",
    "                ent_span = Span(doc, ent.i, ent.i + 1, label=ent.ent_type)\n",
    "                res.append((ent_span, service, relation, root_verb))\n",
    "\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get('https://microsoft-careers.search.windows.net/indexes/microsoft-careers-3/docs?api-version=2017-11-11&search=*', headers={'api-key': '69CA2479ED51AD1AE653457A8FA2B192'})\n",
    "data = res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'recordId': 'a0',\n",
       "  'data': {'text': \"Are you looking for an opportunity to make an impact delivering a new Dynamics 365 AI product to customers? Do you enjoy partnering and collaborating with multiple disciplines and enabling technology teams to deliver compliant solutions on time, with quality to customers? Do you enjoy planning and managing technology projects, leveraging data to make project level decisions? Microsoft Dynamics AI Center of Excellence group is searching for a Senior Program Manager who is looking for an opportunity to project manage complex and high value projects/releases across the product suite and team.In this role, you will have the opportunity to lead and coordinate the Planning, Execution and Release activities for a new product. You’ll work closely with our engineering, compliance, marketing and field teams to define project plans and ensure milestones are completed on time, with quality. This role involves leveraging data to make informed decisions about project health, as well as communicating status to project stakeholders. You will have direct exposure to leadership and the benefit of working with a great team of engineers and program managers who are equally committed to delivering innovative cloud business solutions for our customers.The successful candidate is a person with project management and cross-group collaborations skills, with the ability to schedule and prioritize work against multiple deadlines and the ability to direct teams to success. The ability to thrive in an ambiguous and dynamic environment are essential. Responsibilities Define or evolve existing frameworks for project planning, execution and release activitiesDrive regular project meetings and ensure issues, action items and risks are tracked and have mitigation and resolution plans in placeProvide project status updates to stakeholdersWork closely with other Release Managers to align release activities across the platform and app suiteBuild and maintain project health dashboards and provide awareness with respect to project health metricsFacilitate the definition and completion of Release Criteria for major milestonesHelp bridge the gap across project stateholders and dependencies. Qualifications Required:5+ years of IT/software program management or release management experiencePreferred:Bachelor's degree in Computer Science, Computer Information Systems or related field Demonstrated experience shipping SaaS products on a biweekly cycleDemonstrated experience leveraging data analysis to solve complex problems and influence product decisionsFamiliarity with Microsoft secruity and compliance processesKnowledge of General Data Protection Regulation (GDPR) requirements Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings: Microsoft Cloud Background Check. This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter. Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form. Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.\"}},\n",
       " {'recordId': 'a1',\n",
       "  'data': {'text': 'Core Services Engineering builds and manages the critical products and services that Microsoft runs on. We boldly pursue big ideas that power transformational advances at Microsoft and for our customers, while helping Microsoft teams work smarter, faster and more securely every day. Core Services Engineering employees have deep technical and business expertise, customer insights, and a clear point of view that comes from first-hand, large-scale experience with Microsoft and industry solutions. We are engineers, technology leaders and experts, digital transformation change agents, and customer advocates. We have exciting opportunities for you to innovate, influence, transform, inspire and grow within our organization and we encourage you to apply to learn more! We are looking for excellent software engineers that will be part of the Real Estate & Facilities engineering team and contribute to the evolution of end user services. Real Estate & Facilities deals with creating and operating work environments that are comfortable, functional, collaborative and efficient. It enables employees to realize their potential and helps Microsoft achieve greater business results. Microsoft Real Estate & Facilities provides global employees with a wide range of services and programs essential for team collaboration and innovation, including dining/catering, shuttle/commute, meetings, reception/office upgrades/maintenance, and emergency prep and security. As an indicator to the scale of operations, RE&F covers 44 million square feet of office space, encompassing 600+ buildings across ~100 countries.As a software engineer, you will be involved in designing, coding and testing features that enable our RE&F business partners. You will develop software and services at scale, innovating on the latest technologies (Cloud, modern web, Mobile, Social, Security Software and services) while engineering as a devOps in a highly automated environment with Continuous Integration & Deployment as well as instrumenting telemetry that helps us monitor our service and helps diagnose and resolve issues quickly. You will work closely with peers across teams to help build flexible and high-performing components that enable next generation of business services. Responsibilities Responsibilities:End to End engineering (from architecture, to supporting production) accountability for the service.Design and Develop secure, maintainable, accessible, and scalable modern web, windows, and cross-platform mobile solutions using agile methodology and automation driven approaches.Work with the Product owners to understand and influence the service roadmap.Carry out quick technology “spikes” (or proofs of concepts) to create value in new features and service. Qualifications Required Qualifications:3+ years of recent experience with Cloud Services and C# or other object-oriented programming. 2+ years of experience delivering resilient services.1+ years of experience with helping architect or design modern services. Preferred Qualifications:Experience in applying object oriented design patterns practically in the software design.Well-versed and experienced in coding using object oriented languages like C# (or equivalent) - with concepts like threading, delegates, LinQ.Ability to work with relational databases (like SQL server, t-sql) and semi-structured or noSQL databases (like documentDB, Azure Table/Blob storage etc).Experience using automated test frameworks as a part of the engineering.Ability to work independently (self-driven) as well as coordinate and collaborate with other disciplines when needed.Experience with building in telemetry into the products.Experience working in an Agile methodology (Scrum/Kanban/etc.) with the ability to perform multiple roles (Developer, Scrum Master, Service Reliability Engineer).Experience in building mobile applications for iOS and/or Android platform using React Native, Android Studio, Swift, Objective C..Experience with Live Site monitoring and operating in DevOps environment. The ideal candidate will have experience in a team environment, experience running and designing enterprise scale services and platforms, technical depth in cloud platforms, agile development practices, and experience in designing & tuning telemetry. In addition, this position requires an individual who can demonstrate the ability to ensure highly resilient and scalable service designs through partnership with other members of the service team. #CSEO #EUSEJOBS Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form. Benefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.'}}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = []\n",
    "\n",
    "for i, v in enumerate(data['value'][1:]):\n",
    "    val = {\n",
    "        'recordId': f'a{i}',\n",
    "        'data': {\n",
    "            'text': v['description']\n",
    "        }\n",
    "    }\n",
    "    vals.append(val)\n",
    "vals[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"values\":[{\"recordId\":\"a0\",\"data\":{\"cloudServices\":[\"Azure Visual Studio App Center\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a1\",\"data\":{\"cloudServices\":[\"Azure Stack\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a2\",\"data\":{\"cloudServices\":[\"Azure Web App for Containers\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a3\",\"data\":{\"cloudServices\":[\"Azure Cognitive Services\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a4\",\"data\":{\"cloudServices\":[\"Azure Databricks\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a5\",\"data\":{\"cloudServices\":[]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a6\",\"data\":{\"cloudServices\":[\"Bing Video Search\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a7\",\"data\":{\"cloudServices\":[\"Azure Network Watcher\"]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a8\",\"data\":{\"cloudServices\":[]},\"errors\":null,\"warnings\":null},{\"recordId\":\"a9\",\"data\":{\"cloudServices\":[]},\"errors\":null,\"warnings\":null}]}'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud_res = requests.post('https://cloudcompetegraph.azure-api.net/ner/v1/azure_cognitive_search', json={'values': vals[:10]})\n",
    "cloud_res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
